{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d508809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30a12ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age  weight_kg  heart_rate  blood_pressure  sleep_hours  \\\n",
      "0  0.622951   0.393258    0.334239        0.332512       0.5450   \n",
      "1  0.836066   0.730337    0.214674        0.305419       0.4375   \n",
      "2  0.459016   0.820225    0.222826        0.325123       0.2675   \n",
      "3  0.229508   0.595506    0.206522        0.493842       0.3750   \n",
      "4  0.688525   0.775281    0.177989        0.317734       0.5000   \n",
      "\n",
      "   nutrition_quality  activity_index  smokes  is_fit  height_m       bmi  \\\n",
      "0              0.237        0.744361       0     1.0  0.040816  0.438029   \n",
      "1              0.877        0.548872       0     1.0  0.734694  0.422732   \n",
      "2              0.820        0.258145       0     0.0  0.857143  0.433645   \n",
      "3              0.618        0.671679       0     1.0  0.795918  0.326838   \n",
      "4              0.995        0.959900       1     1.0  0.510204  0.533212   \n",
      "\n",
      "   gender_M  \n",
      "0     False  \n",
      "1     False  \n",
      "2     False  \n",
      "3      True  \n",
      "4     False  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('fitness_dataset_cleaned.csv')\n",
    "df = pd.get_dummies(df, columns=['gender'], drop_first=True)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edfc4dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>blood_pressure</th>\n",
       "      <th>sleep_hours</th>\n",
       "      <th>nutrition_quality</th>\n",
       "      <th>activity_index</th>\n",
       "      <th>smokes</th>\n",
       "      <th>is_fit</th>\n",
       "      <th>height_m</th>\n",
       "      <th>bmi</th>\n",
       "      <th>gender_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.393258</td>\n",
       "      <td>0.334239</td>\n",
       "      <td>0.332512</td>\n",
       "      <td>0.5450</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.744361</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.438029</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.836066</td>\n",
       "      <td>0.730337</td>\n",
       "      <td>0.214674</td>\n",
       "      <td>0.305419</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.548872</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.422732</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.459016</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>0.222826</td>\n",
       "      <td>0.325123</td>\n",
       "      <td>0.2675</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.258145</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.433645</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.595506</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.493842</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.671679</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.326838</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.177989</td>\n",
       "      <td>0.317734</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.959900</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.533212</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  weight_kg  heart_rate  blood_pressure  sleep_hours  \\\n",
       "0  0.622951   0.393258    0.334239        0.332512       0.5450   \n",
       "1  0.836066   0.730337    0.214674        0.305419       0.4375   \n",
       "2  0.459016   0.820225    0.222826        0.325123       0.2675   \n",
       "3  0.229508   0.595506    0.206522        0.493842       0.3750   \n",
       "4  0.688525   0.775281    0.177989        0.317734       0.5000   \n",
       "\n",
       "   nutrition_quality  activity_index  smokes  is_fit  height_m       bmi  \\\n",
       "0              0.237        0.744361       0     1.0  0.040816  0.438029   \n",
       "1              0.877        0.548872       0     1.0  0.734694  0.422732   \n",
       "2              0.820        0.258145       0     0.0  0.857143  0.433645   \n",
       "3              0.618        0.671679       0     1.0  0.795918  0.326838   \n",
       "4              0.995        0.959900       1     1.0  0.510204  0.533212   \n",
       "\n",
       "   gender_M  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3      True  \n",
       "4     False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ab1baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['is_fit']) \n",
    "Y = df['is_fit']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92e81d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c2560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGR = LogisticRegression(max_iter = 1000)\n",
    "LGR.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a11d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7904040404040404\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.86      0.82       225\n",
      "         1.0       0.79      0.70      0.74       171\n",
      "\n",
      "    accuracy                           0.79       396\n",
      "   macro avg       0.79      0.78      0.78       396\n",
      "weighted avg       0.79      0.79      0.79       396\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[194  31]\n",
      " [ 52 119]]\n",
      "\n",
      "F1 Score: 0.7414330218068537\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = LGR.predict(X_test)\n",
    "acc_score = accuracy_score(Y_test, y_pred)\n",
    "cf_report = classification_report(Y_test, y_pred)\n",
    "cfm = confusion_matrix(Y_test, y_pred)\n",
    "f1 = f1_score(Y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy Score:\", acc_score)\n",
    "print(\"\\nClassification Report:\\n\", cf_report)\n",
    "print(\"\\nConfusion Matrix:\\n\", cfm)\n",
    "print(\"\\nF1 Score:\", f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2ef77ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f61d7927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score with Polynomial Features: 0.7651515151515151\n",
      "\n",
      "Classification Report with Polynomial Features:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.82      0.80       225\n",
      "         1.0       0.74      0.70      0.72       171\n",
      "\n",
      "    accuracy                           0.77       396\n",
      "   macro avg       0.76      0.76      0.76       396\n",
      "weighted avg       0.76      0.77      0.76       396\n",
      "\n",
      "\n",
      "Confusion Matrix with Polynomial Features:\n",
      " [[184  41]\n",
      " [ 52 119]]\n",
      "\n",
      "F1 Score with Polynomial Features: 0.7190332326283988\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures()\n",
    "X = poly.fit_transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "LGR = LogisticRegression(max_iter = 1000)\n",
    "LGR.fit(X_train, Y_train)\n",
    "y_pred = LGR.predict(X_test)\n",
    "acc_score = accuracy_score(Y_test, y_pred)\n",
    "cf_report = classification_report(Y_test, y_pred)\n",
    "cfm = confusion_matrix(Y_test, y_pred)\n",
    "f1 = f1_score(Y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy Score with Polynomial Features:\", acc_score)\n",
    "print(\"\\nClassification Report with Polynomial Features:\\n\", cf_report)\n",
    "print(\"\\nConfusion Matrix with Polynomial Features:\\n\", cfm)\n",
    "print(\"\\nF1 Score with Polynomial Features:\", f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8fbc560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for max_iter=1000:\n",
      "Accuracy Score: 0.7651515151515151\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.82      0.80       225\n",
      "         1.0       0.74      0.70      0.72       171\n",
      "\n",
      "    accuracy                           0.77       396\n",
      "   macro avg       0.76      0.76      0.76       396\n",
      "weighted avg       0.76      0.77      0.76       396\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[184  41]\n",
      " [ 52 119]]\n",
      "\n",
      "F1 Score: 0.7190332326283988\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for max_iter=1500:\n",
      "Accuracy Score: 0.7651515151515151\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.82      0.80       225\n",
      "         1.0       0.74      0.70      0.72       171\n",
      "\n",
      "    accuracy                           0.77       396\n",
      "   macro avg       0.76      0.76      0.76       396\n",
      "weighted avg       0.76      0.77      0.76       396\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[184  41]\n",
      " [ 52 119]]\n",
      "\n",
      "F1 Score: 0.7190332326283988\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for max_iter=2000:\n",
      "Accuracy Score: 0.7651515151515151\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.82      0.80       225\n",
      "         1.0       0.74      0.70      0.72       171\n",
      "\n",
      "    accuracy                           0.77       396\n",
      "   macro avg       0.76      0.76      0.76       396\n",
      "weighted avg       0.76      0.77      0.76       396\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[184  41]\n",
      " [ 52 119]]\n",
      "\n",
      "F1 Score: 0.7190332326283988\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [1000, 1500, 2000]:\n",
    "    LGR = LogisticRegression(max_iter = i)\n",
    "    LGR.fit(X_train, Y_train)\n",
    "    y_pred = LGR.predict(X_test)\n",
    "    acc_score = accuracy_score(Y_test, y_pred)\n",
    "    cf_report = classification_report(Y_test, y_pred)\n",
    "    cfm = confusion_matrix(Y_test, y_pred)\n",
    "    f1 = f1_score(Y_test, y_pred)\n",
    "\n",
    "    print(f\"Results for max_iter={i}:\")\n",
    "    print(\"Accuracy Score:\", acc_score)\n",
    "    print(\"\\nClassification Report:\\n\", cf_report)\n",
    "    print(\"\\nConfusion Matrix:\\n\", cfm)\n",
    "    print(\"\\nF1 Score:\", f1)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e369a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for RandomForestClassifier:\n",
      "Accuracy Score: 0.7752525252525253\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.84      0.81       225\n",
      "         1.0       0.77      0.68      0.72       171\n",
      "\n",
      "    accuracy                           0.78       396\n",
      "   macro avg       0.77      0.76      0.77       396\n",
      "weighted avg       0.77      0.78      0.77       396\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[190  35]\n",
      " [ 54 117]]\n",
      "\n",
      "F1 Score: 0.7244582043343654\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for GradientBoostingClassifier:\n",
      "Accuracy Score: 0.7651515151515151\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.84      0.80       225\n",
      "         1.0       0.76      0.66      0.71       171\n",
      "\n",
      "    accuracy                           0.77       396\n",
      "   macro avg       0.76      0.75      0.76       396\n",
      "weighted avg       0.76      0.77      0.76       396\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[190  35]\n",
      " [ 58 113]]\n",
      "\n",
      "F1 Score: 0.7084639498432602\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RFC = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "GBC = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "for i in [RFC, GBC]:\n",
    "    i.fit(X_train, Y_train)\n",
    "    y_pred = i.predict(X_test)\n",
    "    acc_score = accuracy_score(Y_test, y_pred)\n",
    "    cf_report = classification_report(Y_test, y_pred)\n",
    "    cfm = confusion_matrix(Y_test, y_pred)\n",
    "    f1 = f1_score(Y_test, y_pred)\n",
    "\n",
    "    print(f\"Results for {i.__class__.__name__}:\")\n",
    "    print(\"Accuracy Score:\", acc_score)\n",
    "    print(\"\\nClassification Report:\\n\", cf_report)\n",
    "    print(\"\\nConfusion Matrix:\\n\", cfm)\n",
    "    print(\"\\nF1 Score:\", f1)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5401394e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for HistGradientBoostingClassifier with max_iter=100:\n",
      "Accuracy Score: 0.7777777777777778\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.84      0.81       225\n",
      "         1.0       0.77      0.69      0.73       171\n",
      "\n",
      "    accuracy                           0.78       396\n",
      "   macro avg       0.78      0.77      0.77       396\n",
      "weighted avg       0.78      0.78      0.78       396\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[190  35]\n",
      " [ 53 118]]\n",
      "\n",
      "F1 Score: 0.7283950617283951\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for HistGradientBoostingClassifier with max_iter=200:\n",
      "Accuracy Score: 0.7702020202020202\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.83      0.80       225\n",
      "         1.0       0.75      0.70      0.72       171\n",
      "\n",
      "    accuracy                           0.77       396\n",
      "   macro avg       0.77      0.76      0.76       396\n",
      "weighted avg       0.77      0.77      0.77       396\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[186  39]\n",
      " [ 52 119]]\n",
      "\n",
      "F1 Score: 0.723404255319149\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for HistGradientBoostingClassifier with max_iter=300:\n",
      "Accuracy Score: 0.7752525252525253\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.83      0.81       225\n",
      "         1.0       0.76      0.71      0.73       171\n",
      "\n",
      "    accuracy                           0.78       396\n",
      "   macro avg       0.77      0.77      0.77       396\n",
      "weighted avg       0.77      0.78      0.77       396\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[186  39]\n",
      " [ 50 121]]\n",
      "\n",
      "F1 Score: 0.7311178247734139\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HBGC = HistGradientBoostingClassifier(max_iter=100)\n",
    "\n",
    "for i in [100, 200, 300]:\n",
    "    HBGC = HistGradientBoostingClassifier(max_iter=i)\n",
    "    HBGC.fit(X_train, Y_train)\n",
    "    y_pred = HBGC.predict(X_test)\n",
    "    acc_score = accuracy_score(Y_test, y_pred)\n",
    "    cf_report = classification_report(Y_test, y_pred)\n",
    "    cfm = confusion_matrix(Y_test, y_pred)\n",
    "    f1 = f1_score(Y_test, y_pred)\n",
    "\n",
    "    print(f\"Results for HistGradientBoostingClassifier with max_iter={i}:\")\n",
    "    print(\"Accuracy Score:\", acc_score)\n",
    "    print(\"\\nClassification Report:\\n\", cf_report)\n",
    "    print(\"\\nConfusion Matrix:\\n\", cfm)\n",
    "    print(\"\\nF1 Score:\", f1)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94e40266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/salmanjabbar/anaconda3/lib/python3.11/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:15: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=100; total time=   9.8s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=100; total time=  10.1s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=100; total time=  10.2s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=100; total time=  10.2s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=100; total time=  10.3s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=200; total time=  17.5s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=200; total time=  17.7s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=200; total time=  17.7s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=200; total time=  17.4s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=200; total time=  17.1s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=300; total time=  25.9s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=300; total time=  25.9s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=300; total time=  26.6s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=300; total time=  26.7s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=300; total time=  27.0s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=500; total time=  45.7s\n",
      "[CV] END ......learning_rate=0.01, max_depth=5, max_iter=100; total time=  29.5s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=500; total time=  46.6s\n",
      "[CV] END ......learning_rate=0.01, max_depth=5, max_iter=100; total time=  29.5s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=500; total time=  47.4s\n",
      "[CV] END ......learning_rate=0.01, max_depth=5, max_iter=100; total time=  30.3s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=500; total time=  44.9s\n",
      "[CV] END ......learning_rate=0.01, max_depth=3, max_iter=500; total time=  46.4s\n",
      "[CV] END ......learning_rate=0.01, max_depth=5, max_iter=100; total time=  29.4s\n",
      "[CV] END ......learning_rate=0.01, max_depth=5, max_iter=100; total time=  28.3s\n",
      "[CV] END ......learning_rate=0.01, max_depth=5, max_iter=200; total time=  51.3s\n",
      "[CV] END ......learning_rate=0.01, max_depth=5, max_iter=200; total time=  51.9s\n",
      "[CV] END ......learning_rate=0.01, max_depth=5, max_iter=200; total time=  54.7s\n",
      "[CV] END ......learning_rate=0.01, max_depth=5, max_iter=200; total time=  55.1s\n",
      "[CV] END ......learning_rate=0.01, max_depth=5, max_iter=200; total time=  60.0s\n",
      "[CV] END ......learning_rate=0.01, max_depth=5, max_iter=300; total time= 1.3min\n",
      "[CV] END ......learning_rate=0.01, max_depth=5, max_iter=300; total time= 1.3min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 26\u001b[0m\n\u001b[1;32m     16\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m     17\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mhgb,\n\u001b[1;32m     18\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Fit grid search on training data\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Best parameters and score\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    847\u001b[0m         clone(base_estimator),\n\u001b[1;32m    848\u001b[0m         X,\n\u001b[1;32m    849\u001b[0m         y,\n\u001b[1;32m    850\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    851\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    852\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    853\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    854\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    856\u001b[0m     )\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    859\u001b[0m     )\n\u001b[1;32m    860\u001b[0m )\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting  # needed in older sklearn versions\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define model\n",
    "hgb = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Define parameter grid to search\n",
    "param_grid = {\n",
    "    'max_iter': [100, 200, 300, 500],        # number of boosting iterations (trees)\n",
    "    'learning_rate': [0.01, 0.05, 0.1],      # how much each tree contributes\n",
    "    'max_depth': [3, 5, 7, None]             # tree depth (None = unlimited)\n",
    "}\n",
    "\n",
    "# Grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=hgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',   # we optimize for F1 since your dataset is imbalanced\n",
    "    cv=5,           # 5-fold CV\n",
    "    n_jobs=-1,      # use all CPU cores\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit grid search on training data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score (CV):\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "837df67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a61370b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/salmanjabbar/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7803030303030303\n",
      "F1: 0.7339449541284404\n"
     ]
    }
   ],
   "source": [
    "# define base models\n",
    "log_reg = LogisticRegression(max_iter=300)\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "hgb = HistGradientBoostingClassifier(max_iter=300, random_state=42)\n",
    "\n",
    "# create voting ensemble\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_reg), ('rf', rf), ('hgb', hgb)],\n",
    "    voting='soft'  # 'hard' for majority vote\n",
    ")\n",
    "\n",
    "# train on data\n",
    "voting_clf.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(Y_test, y_pred))\n",
    "print(\"F1:\", f1_score(Y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
